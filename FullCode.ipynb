{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import torch\n",
    "import logging\n",
    "from io import BytesIO\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process Traning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/dataset/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the entity_value matches the desired pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_entity_value(value):\n",
    "    pattern = r'^\\d+(\\.\\d+)?\\s+[a-zA-Z]+$'\n",
    "    return bool(re.match(pattern, str(value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_filtered = df[df['entity_value'].apply(is_valid_entity_value)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save  changes to a new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv('/dataset/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy-OCR Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import easyocr\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "logging.basicConfig(filename='image_processing.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info('Starting image processing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Check if GPU is available for EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    logging.info(\"CUDA is available. Using GPU for OCR.\")\n",
    "else:\n",
    "    logging.info(\"CUDA is not available. Using CPU for OCR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure EasyOCR model is downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = easyocr.Reader(['en'], gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load image from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to process a single image and extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(url):\n",
    "    try:\n",
    "        # Load the image from URL\n",
    "        image = load_image_from_url(url)\n",
    "        if image is None:\n",
    "            logging.warning(f\"Failed to load image: {url}\")\n",
    "            return \"\"\n",
    "\n",
    "        # downscale the image for faster processing\n",
    "        image = cv2.resize(image, (600, 600), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Perform OCR\n",
    "        results = reader.readtext(image)\n",
    "\n",
    "        # Extract recognized text\n",
    "        recognized_text = ' '.join([result[1] for result in results])\n",
    "        logging.info(f\"Processed image: {url} - Extracted text: {recognized_text}\")\n",
    "        return recognized_text\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing image {url}: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Training CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = '/dataset/train.csv'  \n",
    "logging.info(f\"Loading CSV file: {input_csv}\")\n",
    "df = pd.read_csv(input_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare image URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = df['image_link'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process images in parallel with progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\"\"] * len(image_urls)\n",
    "batch_size = 10  # Adjust batch size as needed\n",
    "checkpoint_interval = 1000  # Save after every 1000 records\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_image, url, idx): idx for idx, url in enumerate(image_urls)}\n",
    "    for i, future in enumerate(tqdm(as_completed(futures), total=len(futures), desc=\"Processing images\")):\n",
    "        idx, result = future.result()\n",
    "        outputs[idx] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the output to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['output'] = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the updated DataFrame to a new CSV file using a semicolon as delimiter(Comma can interfare with csv files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = '/dataset/Train_output.csv'  # Replace with the desired output path\n",
    "logging.info(f\"Saving updated CSV file: {output_csv}\")\n",
    "df.to_csv(output_csv, index=False, sep='~')\n",
    "logging.info(\"Process completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Run OCR for Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Training CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = '/dataset/test.csv'  \n",
    "logging.info(f\"Loading CSV file: {input_csv}\")\n",
    "df = pd.read_csv(input_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare image URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = df['image_link'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process images in parallel with progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\"\"] * len(image_urls)\n",
    "batch_size = 10  # Adjust batch size as needed\n",
    "checkpoint_interval = 1000  # Save after every 1000 records\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_image, url, idx): idx for idx, url in enumerate(image_urls)}\n",
    "    for i, future in enumerate(tqdm(as_completed(futures), total=len(futures), desc=\"Processing images\")):\n",
    "        idx, result = future.result()\n",
    "        outputs[idx] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the output to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['output'] = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the updated DataFrame to a new CSV file using a semicolon as delimiter(Comma can interfare with csv files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = '/dataset/Test_output.csv'  # Replace with the desired output path\n",
    "logging.info(f\"Saving updated CSV file: {output_csv}\")\n",
    "df.to_csv(output_csv, index=False, sep='~')\n",
    "logging.info(\"Process completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now have Train_output.csv with OCR data that we will use to train our model on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit conversion dictionary (Expansion of Constants.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_conversions = {\n",
    "    'cm': 'centimetre', 'centimeter': 'centimetre',\n",
    "    'ft': 'foot', 'foot': 'foot', \"'\": 'foot', 'feet': 'foot',\n",
    "    'in': 'inch', 'inch': 'inch', '\"': 'inch',\n",
    "    'm': 'metre', 'meter': 'metre',\n",
    "    'mm': 'millimetre', 'millimeter': 'millimetre',\n",
    "    'yd': 'yard', 'yard': 'yard',\n",
    "    'g': 'gram', 'gram': 'gram',\n",
    "    'kg': 'kilogram', 'kilogram': 'kilogram',\n",
    "    'µg': 'microgram', 'mcg': 'microgram', 'microgram': 'microgram',\n",
    "    'mg': 'milligram', 'milligram': 'milligram',\n",
    "    'oz': 'ounce', 'ounce': 'ounce',\n",
    "    'lb': 'pound', 'lbs': 'pound', 'pound': 'pound',\n",
    "    't': 'ton', 'ton': 'ton', 'short ton': 'ton', 'long ton': 'ton',\n",
    "    'kV': 'kilovolt', 'kilovolt': 'kilovolt',\n",
    "    'mV': 'millivolt', 'millivolt': 'millivolt',\n",
    "    'V': 'volt', 'volt': 'volt',\n",
    "    'kW': 'kilowatt', 'kilowatt': 'kilowatt',\n",
    "    'W': 'watt', 'watt': 'watt',\n",
    "    'cL': 'centilitre', 'centiliter': 'centilitre',\n",
    "    'ft³': 'cubic foot', 'cu ft': 'cubic foot', 'cubic foot': 'cubic foot',\n",
    "    'in³': 'cubic inch', 'cu in': 'cubic inch', 'cubic inch': 'cubic inch',\n",
    "    'cup': 'cup',\n",
    "    'dL': 'decilitre', 'deciliter': 'decilitre',\n",
    "    'fl oz': 'fluid ounce', 'oz fl': 'fluid ounce', 'fluid ounce': 'fluid ounce',\n",
    "    'gal': 'gallon', 'imperial gallon': 'gallon', 'gallon': 'gallon',\n",
    "    'imp gal': 'imperial gallon', 'imperial gallon': 'imperial gallon',\n",
    "    'L': 'litre', 'liter': 'litre', 'litre': 'litre',\n",
    "    'µL': 'microlitre', 'mcL': 'microlitre', 'microliter': 'microlitre',\n",
    "    'mL': 'millilitre', 'ml': 'millilitre', 'milliliter': 'millilitre',\n",
    "    'pint': 'pint',\n",
    "    'qt': 'quart', 'quart': 'quart', 'ter': 'tre'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/dataset/Train_output.cs', delimiter='~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input and output texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [f\"extract entity_value: {row['entity_name']} {row['output']}\" for _, row in data.iterrows()]\n",
    "output_texts = [row['entity_value'] for _, row in data.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the T5-small model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(input_texts, truncation=True, padding=True, max_length=128)  # Consider reducing max_length if possible\n",
    "labels = tokenizer(output_texts, truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom callback for status tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatusCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        logs = logs or {}\n",
    "        print(f\"Step: {state.global_step}, Loss: {logs.get('loss', 'N/A')}, Learning Rate: {logs.get('learning_rate', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up training arguments and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # Reduced batch size to handle larger datasets and faster processing\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    fp16=True,  \n",
    "    dataloader_num_workers=2,  \n",
    "    save_steps=1000,  # Saving model checkpoint every 1000 steps\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    callbacks=[StatusCallback()]  # Add the custom callback here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./fine_tuned_t5_model')\n",
    "tokenizer.save_pretrained('./fine_tuned_t5_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Predictions to Output FIle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Test_Output for Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/dataset/Test_output.cs', delimiter='~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fine-tuned model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './fine_tuned_t5_model'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "model.to('cuda')  # Move model to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Dataset class for efficient batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        entity_name = row['entity_name']\n",
    "        ocr_text = row['output']\n",
    "        text = f\"extract entity_value: {entity_name} {ocr_text}\"\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EntityDataset(data)\n",
    "batch_size = 64  \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "predictions = []\n",
    "for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "    inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = {key: val.to('cuda') for key, val in inputs.items()}  \n",
    "    outputs = model.generate(**inputs)\n",
    "    batch_predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    # Post-process predictions\n",
    "    for predicted_value in batch_predictions:\n",
    "        if predicted_value.strip() == \"\":\n",
    "            predictions.append(\"\")  # Return an empty string if no value was found\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            value, unit = predicted_value.split()\n",
    "            normalized_unit = unit_conversions.get(unit, unit)\n",
    "            predictions.append(f\"{value} {normalized_unit}\")\n",
    "        except ValueError:\n",
    "            # If split fails, it means the output format is not as expected\n",
    "            predictions.append(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add predictions to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the output CSV with 'index' and 'prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = data[['index', 'prediction']]\n",
    "output_data.to_csv('/dataset/OUTPUT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Sanity check using src/sanity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python sanity.py --test_filename ../dataset/test.csv --output_filename ../dataset/test_out.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
